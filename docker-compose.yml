version: '3.8'

services:
  # LLama API Service
  llama-api:
    image: llama-api  # Replace with your actual image name if different
    container_name: llama-api
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models  # Mount models directory if needed
    networks:
      - llm-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # NeMo Guardrails Service
  nemo-guardrails:
    build:
      context: ./nemo_guardrails
    image: nemo-guardrails
    container_name: nemo-guardrails
    environment:
      - LLAMA_API_URL=http://llama-api:8000/generate
    ports:
      - "8080:8080"
    networks:
      - llm-net
    depends_on:
      - llama-api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Inference UI Service
  inference-ui:
    build:
      context: ./inference-ui
    image: inference-ui
    container_name: inference-ui
    environment:
      - PORT=3000
      - LLAMA_API_URL=http://nemo-guardrails:8080  # Don't include /generate or /chat
    ports:
      - "3000:3000"
    networks:
      - llm-net
    depends_on:
      - nemo-guardrails

networks:
  llm-net:
    name: llm-net
    external: true
      